\documentclass[sigconf, review=true]{acmart}

\usepackage{booktabs} % For formal tables

\graphicspath{{../../figs/}}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
%\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 2017}{El
%  Paso, Texas USA} 
%\acmYear{1997}
%\copyrightyear{2016}


%\acmArticle{4}
%\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{CleverName: \\
A Zero-Power All-Optical Convolutional Neural Network}
% \titlenote{Produces the permission block, and
%  copyright information}
% \subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


\author{Author Author}
% \authornote{Dr.~Trovato insisted his name be first.}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Stanford University}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin} 
  % \state{Ohio} 
  % \postcode{43017-6221}
}
%\email{jchang10@stanford.edu}

\author{Author Author}
\affiliation{%
  \institution{Stanford University}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin} 
  % \state{Ohio} 
  % \postcode{43017-6221}
}
%\email{gordon.wetzstein@stanford.edu}

\author{Author Author}
\affiliation{%
  \institution{Stanford University}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin} 
  % \state{Ohio} 
  % \postcode{43017-6221}
}
%\email{gordon.wetzstein@stanford.edu}

% The default list of authors is too long for headers.
% \renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
Abstract here.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


% \keywords{ACM proceedings, \LaTeX, text tagging}

%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{This is a teaser}
%  \label{fig:teaser}
%\end{teaserfigure}

\maketitle


% \input{samplebody-conf}

%%%=============================================%%%
%%%=============================================%%%

\section{Introduction}
Deep neural networks have found success in a wide variety of applications, ranging from computer vision to natural language processing to game playing \cite{lecun2015deep}. Since the explosion of interest following the achievements of convolutional neural networks on ImageNet classification, deep learning has transformed algorithms in both academic and commercial environments. While accuracy has improved to a remarkable level, the number of parameters and connections has grown dramatically, and the power requirements to build and use these networks have increased correspondingly. 

While the training phase of learning parameter weights is often considered the slow stage, large models also demand significant energy during inference due to millions of repeated memory references. For example, AlphaGo has a power consumption of approximately 300 W. To this end, there is a large effort to develop new software methods and specialty hardware for improved efficiency. Algorithms for improved efficiency include pruning, quantization, low rank, parallelization, mixed precision, and model distillation. Compressed models have demonstrated preserved accuracy with much fewer parameters \cite{han2015deep, iandola2016squeezenet}. On the hardware front, there are now specialized processing units for deep learning, such as TrueNorth, Movidius's USB-based neural compute stick (NCS), and Google's tensor processing unit (TPU). Despite all these efforts, it remains difficult for embedded systems such as mobile vision, autonomous vehicles/robots, and wireless smart sensors to deploy CNNs due to stringent constraints on power and bandwidth. 

Optical computing has been tantalizing for its high bandwidth and inherently parallel processing. If we can come up with scalable optical configurations that together act as a framework for an optical CNN, this would be of interest to computer vision, robotics, machine learning, and optics communities. Optical neural networks (ONNs) could also potentially exploit wave optics for complex-valued neural networks and new types of non-linearities that are currently unavailable to digital computation. Linear transformations can be performed with lens systems or interferometer meshes \cite{shen2017deep}. Optical nonlinearities include saturable absorption and bistability. Liquid crystal light valves have been used as optical thresholding devices \cite{saxena1995adaptive}. Convolutions are commonly performed with PSF engineering. We use these to design a model for cascaded parallel convolutions with sandwiched nonlinearity layers.

In this paper, we follow the vein of computational photography to create we zero-power, all-optical convolutional neural network for image classification. We choose a simple classification task, e.g. classify handwritten digits, and build a prototype that performs inference on projected images. We precompute the weights by training with the MNIST dataset on a computer, then fabricate the optical elements accordingly. We compare performance with the same inference performed on the computer. Here we demonstrate proof-of-concept with bulk optics and free-space propagation, but we recommend photonic integrated circuits for scalability. Photonic circuits with up to 4,096 optical devices have been demonstrated \cite{sun2013large}, and there have also been new three-dimensional photonic integrations that could enable larger networks \cite{rechtsman2013photonic}. Combination of these next-generation large-scale photonic circuits with compressed deep learning models could provide a potential route for high performance ONNs.

To summarize, we make the following contributions:
\begin{itemize}
\item We propose an optical toolbox of building blocks for convolutional neural networks. 
\item We build a zero-power, all-optical two-layer network with precomputed weights for image classification. 
\item We evaluate against the computer implementation of the same network and show that we achieve similar accuracy.
\end{itemize}

\textit{Overview of limitations.} We are limited to non-negative values since we are working with light intensities. This may be avoided with coherent processing. We also hope to proceed without normalization. We focus here on inference and do training on the computer. In our current implementation without active elements, we lose flexibility to update the network. However, in fixed applications, this is not a problem. Photonic circuits remain expensive to fabricate as they are not used in common consumer applications.


%%%=============================================%%%
%%%=============================================%%%

\section{Related works}

\paragraph{CNNs and architecture variations.} Artificial neural networks were proposed in X. Early networks were composed of fully connected layers with nonlinear activation functions in between, inspired by the canonical biological neuron and its thresholded activation. Convolutional layers were popularized by LeCun and ... in image classification CITE. Convolutional layers allow for weight sharing... Since then, deeper, more complex, etc. Since an optical implementation of a CNN comes with certain constraints and challenges, we wanted to see what types of CNNs have been explored with non-standard architectures that may align with physical designs. Omission of fully connected layers, i.e. fully convolutional with global average pooling at the top layer has proven to be successful in \cite{lin2013network,iandola2016squeezenet}. Analysis of CNN operations in the Fourier domain, introducing spectral pooling and regularization \cite{rippel2015spectral}. Relevant because we can also access optical Fourier plane. We also note the work in the complex-valued deep neural networks \cite{trabelsi2017deep}, as coherent optical signals may be an effective means of propagating complex-valued data.

\paragraph{Optical computing} High bandwidth, but high cost. Optoelectronics and fully optical. Optical solutions to NP-complete problems that are faster than electronic computation \cite{wu2014optical}.  In the early days of CNNs, there was also momentum for optical implementation, optical neural networks (ONNs). Adaptive optical network using volume holographic interconnects in photorefractive crystals \cite{psaltis1988adaptive}. Hybrid optoelectronic network with feedback loop, computer for subtraction and thresholding operations \cite{lu1989two}. Optical thresholding perceptron implemented with liquid crystal light valves (LCLV) \cite{saxena1995adaptive}. There has also been much development in photonic computing. Recently, two-layer fully connected NN demonstrated with intermediate simulated nonlinearity units on 1D data \cite{shen2017deep}. However, this required photodetection and reinjection, and it did not involve convolutional layers. Optalysys? We do not work with photonic circuits here, but we think they may be worth exploring for larger networks.

\paragraph{Computational cameras.} Computational photography has some intersection with optical computing in that they may perform some operations on the input signal optically, but they are also distinct in that they work with spatially organized inputs that come from physical world (incoherent light). Coded apertures and PSF engineering can perform filtering [CITE]. Optical correlators that essentially perform template matching on images have been explored for optical target detection and tracking \cite{manzur2012optical, javidi1995optical}. Somewhat similar to our goal is focal plane processing, which refers to the incorporation of image processing on the sensor chip, eliminating or reducing the need to shuttle full image data to a processor. These chips have been designed to detect edges and orientations and to perform wavelet or discrete cosine transforms \cite{gruev2002implementation} [RedEye]. Most of these approaches still rely on electronic computation on the image sensor chip, whereas our goal is all-optical implementation with no additional power input. Chen et al. use optically designed angle sensitive pixels, photodiodes with integrated diffraction gratings producing Gabor wavelet impulse responses, to approximate the kernels of the first layer of a typical convolutional neural network \cite{chen2016asp}. However, this design is limited to a fixed set of convolution kernels, and the output still has to be shuttled to a computer for further processing. Our goal is to build an end-to-end classification system with flexible and rearrangeable optical units that allows for custom optical CNNs.

%%%=============================================%%%
%%%=============================================%%%
\pagebreak

\section{ONN toolbox}

Here we describe proposed optical building blocks corresponding to common layers in a CNN. In a standard feed-forward CNN, information is passed in a single direction through a sequence of layers. Cycles, loops, interacting networks, etc. can be incorporated in more complicated architectures that may be interesting to explore in the future. For now, we will focus on the most essential components that define a CNN in the context of an image classification task.

\subsection{Convolutional layer}
A CNN typically begins with a convolutional layer, which essentially performs pattern matching with a set of learnable visual filters. A standard convolutional layer takes an input volume of depth $C_\text{in}$, performs a series of correlations with a set of $c_\text{out}$ kernels each with depth $C_\text{in}$, and outputs a new volume of depth $C_\text{out}$. The correlation of the kernel across the width and height of the input volume produces a 2D "activation map", and stacking the $C_\text{out}$ activation maps for all kernels forms the output volume of depth $C_\text{out}$. Hyperparameters include the spatial extent of the kernel $F$, the stride with which the kernel is applied, and the padding of the input volume. Here we assume a stride of $1$, meaning the kernel is shifted by one pixel at a time, and zero-padding such that the output volume has the same height and width as the input.

\subsubsection{Tiled PSFs} In optical systems, image formation is often modeled as a spatially invariant convolution of the scene with the point spread function (PSF) of the system:
\begin{equation}
I_\text{out}  = I_\text{in} * \text{PSF}
\end{equation}

Let us generalize this scene as an input $I$ that can also be a real image relayed by a lens. This simple case can be viewed as a convolutional layer with $C_\text{in} = C_\text{out} = 1$ and the flipped PSF as the single kernel. We will also refer to the flipped PSF as the kernel since the flipping is trivial. Now suppose we want $C_\text{out} = n, n >1$. By spatially tiling the multiple kernels as the PSF of the system, the output becomes the convolution of the input image with multiple kernels, but now the $n$ outputs are tiled laterally instead of stacked in depth. Consideration can be taken to ensure these outputs are non-overlapping by adjusting the shifts $\Delta x$, if desired. 
\begin{equation}
\text{PSF} = \sum_{i = 1}^n W_i * \delta(x - i\Delta x)
\end{equation}
\begin{equation}
I_\text{out} = I_\text{in} * PSF = \sum (I_\text{in} * W_i) * \delta(x - i\Delta x)
\end{equation}

The next important extension is to incorporate $C_\text{in} = m, m > 1$. If we needed to exactly imitate the digital CNN, we would need $m$ different kernels for each of the $m$ input channels. This could potentially be implemented with many of the single channel modules in parallel, with the addition of a relay that sums $m$ outputs that correspond to the different depth slices of the same kernel, but this type of setup may be prohibitively complicated to build. If we slightly relax our requirements, we could again rely on Fourier optics to perform the summation. Now suppose we tile the input images in addition to the kernels:
$$I = \sum_{j = 1}^m$$

This combination of tiled images and tiled PSFs results in some cycling of the kernels, but . 

\subsubsection{Optimized phase masks} Instead of first optimizing the PSFs and then separately optimizing a phase mask to best produce these PSFs, end-to-end optimization. When optimizing for the phase mask as a whole, we realized we no longer need to think about tiling many small PSFs, but rather just optimize for one large PSF.  

\subsection{Nonlinear activation layer}
Nonlinear activation layers are crucial components in the neural network toolbox that allow for modeling of nonlinear relationships between input and output variables. Most commonly used is a rectified linear unit (ReLU), that simply sets all negative values to 0: $\text{ReLU}(x) = \max\{0, x\}$. In an optical intensity-based system, there are no non-negative values, so the standard ReLU function does not directly apply. However, if we consider the purpose of the ReLU layer to zero out some fraction of the neurons below a threshold response level, then we hypothesize that we can accomplish a similar effect by shifting this threshold to a positive value. 

This nonlinear behavior translates to an ideal optical element that is fully opaque when incident light is low intensity and fully transmissive when incident light is above a threshold. A perfectly binary switch is difficult to physically realize, so instead we sought a material that would be less transmissive to lower incident intensities and become more transmissive at higher incident intensities. In fact, this type of nonlinear response is remniscent of the PReLU) \cite{he2015delving}. (Swish) \cite{ramachandran2017searching}

Bacteriorhodopsin


\subsection{Fully-connected layer}
The fully connected layer is so named because every input neuron is connected to every output neuron.
The input is flattened into a single vector and multiplied with a matrix of size $D_\text{out} \times D_\text{in}$, where $D_\text{in} = H \times W \times C_\text{in}$.

Spatially-varying convolution with spatial extend equal to the size of the . Maybe not necessary as global average pooling (GAP) has been successful.

\subsection{Pooling layer}
Pooling layers can be inserted, commonly between convolutional layers, to reduce spatial size and consequently computation. Pooling operations, for example "maximum", operate on each depth slice independently. The same hyperparameters of spatial extent $F$ and stride $S$ also apply, though the most commonly seen pattern is $F=2, S=2$.

\subsubsection{Average pooling}
While it is not obvious how to take the spatial maximum of an optical signal without active sensing, average pooling can be approximated with a reduction in the spatial resolution of the 

\subsubsection{Spectral pooling}
Spectral pooling is another interesting concept that carries over easily to our ONN setup. can be viewed as a generalization of average pooling. 

\subsection{Other considerations}
We have designed these optical building blocks to have input and output in the same format. This allows an arbitrary chaining of blocks to create the desired CNN architecture. Not all of these designs are easily scalable to the same sizes. All weights will be non-negative. It is possible to include negative values when coherent signals are used, but we do not explore that here. We will evaluate the implications of our system constraints in the next section.

%\begin{figure*}
%\includegraphics[width = .9\linewidth]{layers.pdf}
%\caption{Layers.}
%\end{figure*}

%%%=============================================%%%
%%%=============================================%%%
\newpage

\section{Understanding ONNs}
Here we aim to understand the effect of imposing the previously mentioned constraints on the performance of a neural network. We start with a standard network and add in constraints of nonnegative weights, missing or nonnegative bias, limited set of nonlinear activation functions, etc. We set up these models in TensorFlow. Non-negativity constraints included by XXXXX. Biases simply removed from each layer. ReLU replaced by the model of XXXXXX. N iterations of XXXX optimizer (learning rate = X). 

\paragraph{Case 1: Fully connected NN for 2D classification} Classification of 2D points.
\begin{equation}
C_{5\times 5}^{32} \ \rightarrow \  C_{5\times 5}^{64} \  \rightarrow \  FC \  \rightarrow \  \text{Softmax}
\end{equation}

%\begin{figure}
%\includegraphics[width = \columnwidth]{2d.pdf}
%\caption{2D classification}
%\end{figure}

\paragraph{Case 1: CNN for image classification} To compare performance with modified convolutional layers. This model can be larger. Image classification of MNIST or CIFAR. Network architecture is:


Original error rate is 0.01\%. 



\begin{table}[hb]
\begin{center}
\begin{tabular*}{\columnwidth} {l || c | c | c | c | c | c | c | c }
Constraints	& 1 \ & \  2 \ & \ 3 	\	& \ 4 \ & \ 5 \ & \ 6 \ &  \ 7 \ & \ 8 \  \\ \hline \hline
Non-neg. 	&   	&  \checkmark	& \checkmark  & \checkmark & \checkmark &  \checkmark & & \\ \hline
No biases  	&   	&  			&  \checkmark	& \checkmark & \checkmark & \checkmark & & \\ \hline
Non-ReLU 	&   	&  	&   		& \checkmark & \checkmark & \checkmark & & \\ \hline
Spectral pooling &   	&  	&   		& \checkmark & \checkmark & \checkmark & & \\ \hline
Cycled kernels	&   	&  	&   		&  & \checkmark & \checkmark & & \\ \hline
\dots		&   	&  	&   		&  & & \checkmark & &  \\ \hline\hline
1000 epochs			& .1\%	&   	&  		&  & &  & & \\ \hline 
10000 epochs	&  .5\% 	&  	&   		&  & & & & \\ \hline
\end{tabular*}
\end{center}
\caption{MNIST classification error rate with various NN constraints.}
\end{table}

%%%=============================================%%%
%%%=============================================%%%
\pagebreak 
\section{Implementation}

\subsection{Network architecture}
For our prototype, we limit to two convolutional layers and one fully connected layer. We train a small n-layer network to perform classification. 

\subsection{Optical protoype}
We fabricate phase masks / print amplitude masks. Build optical setup on benchtop. Project images for inference. 
\begin{figure}
\includegraphics[width = \columnwidth]{optical}
\caption{setup}
\end{figure}

%%%=============================================%%%
%%%=============================================%%%
\section{Results}
These are the results captured with the prototype.


%%%=============================================%%%
%%%=============================================%%%

\section{Discussion}
- brief summary

- extended discussion of coherent vs incoherent light, how to train optically, and other aspects that may be interesting (this discussion could also include the limitations)

- future work

%%%=============================================%%%
%%%=============================================%%%

\section{Conclusion}
We hope this will inspire more research in the area. 

%%%=============================================%%%
%%%=============================================%%%
\pagebreak

\bibliographystyle{ACM-Reference-Format}
\bibliography{../bibliography} 

\end{document}
