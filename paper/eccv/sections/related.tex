\paragraph{Efficient convolutional neural networks.} 
Since our work is motivated by the potential of optics to increase the effiiciency of CNN applications, we first review algorithms and electronic hardware also designed to address this challenge. Pruning, trained quantization, huffman encoding, and altered architectural design have been successfully used to compress CNN models,  preserving AlexNet-level accuracy on ImageNet even with 510$\times$ less memory usage and 50$\times$ fewer parameters \cite{han2015deep,iandola2016squeezenet}. On the hardware front, there are now specialized processing units for deep learning, such as TrueNorth, Movidius's USB-based neural compute stick (NCS), and Google's tensor processing unit (TPU). All of these are complementary to our approach, which still requires offline training to optimize the optical components. Other inference-focused efforts aimed at embedded vision applications have tried to incorporate a portion of the image processing on the sensor chip, eliminating or reducing the need to shuttle full image data to a processor. Analog circuitry has been used to detect edges and orientations, to perform wavelet or discrete cosine transforms, and even to execute layers of a CNN \cite{gruev2002implementation,likamwa2016redeye}. These approaches still rely on electronic computation on the image sensor chip, whereas our goal is to push more of the computation into optical hardware that requires no power input. 

\paragraph{Computational cameras.} Optical computation is attractive because it offers inherent parallelism and high interconnectivity, both of which are encountered when passing signals through neural networks. In the computational imaging community, many system designs already exploit the physical propagation of light through custom optics to encode information about a scene that would be lost in a standard 2D image capture. Computational cameras have been created to record depth, light fields, light transport, and more with a toolbox including coded apertures, lenses and lenslets, active illumination, and wavefront shapers \cite{ng2005light,levin2007image,mcguire2007optical,o2010optical,chang2016variable}. In this work, we propose a computational imaging system modeled after a CNN that assists in performing classification of input images. We begin by learning an optical correlator consisting of a single convolutional layer that essentially performs template matching on images, as has been explored for optical target detection and tracking \cite{manzur2012optical,javidi1995optical}, and then expand beyond a single matched filter in hybrid optoelectronic and fully optical designs.

\paragraph{Optical neural networks.}   
The concept of an optical neural network (ONN) captured the attention of many in the late 1980s to mid-1990s, primarily due to the capability of optics to perform the expensive matrix multiply of a fully connected layer. In 1985, an optoelectronic implementation of the Hopfield model, a basic model of a recurrent neural network, was created with 1D LED array input signals and a binary transmission mask \cite{farhat1985optical}. This model divided the weight matrix into two parts, positive and negative, and required electronics for subtraction of the two parts and signal thresholding. Psaltis et al. further explored the potential of dynamic photorefractive crystals to store neural network weights, which could allow for optical backpropagation-based learning in ONNs \cite{psaltis1988adaptive}. Meanwhile, the optoelectronic network of a Hopfield model was extended to 2D signals by partitioning the pixels of a liquid crystal television to store an array of smaller 2D patterns \cite{lu1989two}. Furthermore, an optical thresholding perceptron was implemented with liquid crystal light valves (LCLV), which disposed of the need to convert between optical and electronic signals between layers \cite{saxena1995adaptive}. We draw on some of these insights for the design of our optical CNN. A more extensive overview of the varied implementations of ONNs can be found in \cite{denz2013optical}.

Despite the progress in this area, as neural networks fell out of the spotlight, the demand for ONNs also waned. However, with the resurgence of CNNs that are far more powerful and computationally expensive than before, there is renewed interest in optical computing \footnote{Fathom Computing (\url{fathomcomputing.com}), Lightelligence (\url{lightelligence.ai}), Optalysis (\url{optalysys.com)}}. Recent works that connect efforts of the last century to modern hardware include a two-layer fully connected neural network based on programmable photonic circuits \cite{shen2017deep} and a recurrent neural network with DMD-based weights \cite{bueno2017reinforcement}. However, none of the ONNs mentioned previously involve convolutional layers, which have become essential in computer vision applications. The ASP Vision system approaches the task of designing a hybrid optoelectronic CNN, using angle sensitive pixels to approximate the first convolutional layer of a typical CNN, but it is limited to a fixed set of convolution kernels \cite{chen2016asp}. Our goal is to design a system with optimizable optical elements to demonstrate low-power inference by a custom optical or optoelectronic CNN.

